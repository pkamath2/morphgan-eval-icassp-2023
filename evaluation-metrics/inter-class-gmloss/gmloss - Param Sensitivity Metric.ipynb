{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fec939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys, librosa\n",
    "from librosa import display\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "from IPython.display import Audio \n",
    "import IPython\n",
    "\n",
    "import cdpam\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import scipy.spatial as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import collections as c\n",
    "\n",
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "\n",
    "import soundfile as sf\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242eaefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10c3d169",
   "metadata": {},
   "source": [
    "### Per Antognini et al. (Synthesizing Diverse, High-Quality Audio Textures) - https://arxiv.org/abs/1806.08002:\n",
    "#### Input representation \n",
    "1. Ensure hop_length is less than half of win_length\n",
    "2. Get absolute value of stft, add 1 and take natural logarithm. \n",
    "   Adding 1 guarantees that the log-spectrogram is finite and positive\n",
    "\n",
    "#### Network architecture\n",
    "1. 6 CNN layers, with filter sizes 2^n\n",
    "2. Each layer with 512 filters\n",
    "3. Each filter randomly drawn from Glorot initialization (In PyTorch - torch.nn.init.xavier_uniform) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74774e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available = False\n"
     ]
    }
   ],
   "source": [
    "BOOTSTRAP_ITERS = 10 #To generate SEMs and Means.\n",
    "\n",
    "N_PARALLEL_CNNS=6\n",
    "\n",
    "N_FFT = 512 \n",
    "K_HOP = 128 \n",
    "N_FREQ= 257\n",
    "N_FILTERS = 512\n",
    "\n",
    "possible_kernels = [2,4,8,16,64,128,256,512,1024,2048]\n",
    "filters = [0]*N_PARALLEL_CNNS\n",
    "for j in range(N_PARALLEL_CNNS):\n",
    "    filters[j]=possible_kernels[j]\n",
    "    \n",
    "\n",
    "use_cuda = torch.cuda.is_available() #use GPU if available\n",
    "print('GPU available =',use_cuda)\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c678a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_spectrum(filename):\n",
    "    #x, fs  = sf.read(filename)\n",
    "    x, fs  = librosa.load(filename, sr=16000)\n",
    "    R = np.abs(librosa.stft(x, n_fft=N_FFT, hop_length=K_HOP, win_length=N_FFT,  center=False))  \n",
    "    R += 1\n",
    "    R = np.log(R)\n",
    "    return R,fs\n",
    "\n",
    "\n",
    "'''\n",
    "Make input such that frequency bins of Spectrogram are channels.\n",
    "i.e. if Spectrogram shape is 257 X 247 => Input to convolution should be 1 X 257 X 1 X 247 \n",
    "(PyTorch uses batch X channels X Height X Width)\n",
    "'''\n",
    "def prepare_input(filename):\n",
    "    R, fs = read_audio_spectrum(filename)\n",
    "    a_style = np.ascontiguousarray(R[None,None,:,:])\n",
    "    a_style = torch.from_numpy(a_style).permute(0,2,1,3) \n",
    "    converted_img = Variable(a_style).type(dtype)\n",
    "    return converted_img\n",
    "\n",
    "# Glorot initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1e7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This variable lets you 'tap' into the style_net to retrieve Gram Matrices at the layer matching that name.\n",
    "style_layers_default = ['relu_2']\n",
    "\n",
    "\n",
    "class style_net(nn.Module):\n",
    "    \"\"\"Here create the network you want to use by adding/removing layers in nn.Sequential\"\"\"\n",
    "    def __init__(self, num_channels, num_filters, filter_size):\n",
    "        super(style_net, self).__init__()\n",
    "        self.layers = nn.Sequential(c.OrderedDict([\n",
    "                            ('conv1',nn.Conv2d(num_channels, num_filters, kernel_size=(1,filter_size),bias=False)),\n",
    "                            ('relu1',nn.ReLU())]))\n",
    "\n",
    "            \n",
    "    def forward(self,input):\n",
    "        out = self.layers(input)\n",
    "        return out\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    '''Compute the feature correlations'''\n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()         \n",
    "        features = input.view(b, a * c * d)\n",
    "        features2=features.unsqueeze(0)\n",
    "        G = torch.matmul(features2, torch.transpose(features2, 1,2))\n",
    "        return G.div(a * c * d)\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "Initialize parallel CNNs as suggested by Antognini et al.\n",
    "Typical values:\n",
    "num_cnns = N_PARALLEL_CNNS\n",
    "per_cnn_num_channels = N_FREQ\n",
    "per_cnn_num_filters = N_FILTERS\n",
    "per_cnn_filter_sizes = [2,4,8,16,64,128]\n",
    "'''\n",
    "def initialize_parallel_cnns(num_cnns, per_cnn_num_channels, per_cnn_num_filters, per_cnn_filter_sizes):\n",
    "    cnnlist=[] \n",
    "    for j in range(num_cnns) :\n",
    "        cnn = style_net(per_cnn_num_channels, per_cnn_num_filters, per_cnn_filter_sizes[j])\n",
    "        cnn.apply(lambda x: weights_init(x))\n",
    "        for param in cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        if use_cuda:\n",
    "            cnn = cnn.cuda()\n",
    "\n",
    "        cnnlist.append(cnn)\n",
    "        \n",
    "    return cnnlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3027f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram(cnn, formatted_input, style_layers=style_layers_default):\n",
    "    \n",
    "    result = []\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    \n",
    "    model = nn.Sequential()\n",
    "    layer_list = list(cnn.layers)\n",
    "    gram = GramMatrix()\n",
    "     \n",
    "    i = 1  \n",
    "    for layer in layer_list:\n",
    "        if isinstance(layer, nn.Conv2d): \n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers: \n",
    "                target_feature = model(formatted_input).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= MinMaxScaler().fit_transform(target_feature_gram)\n",
    "                \n",
    "                result.append((target_feature_gram))\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers:\n",
    "                target_feature = model(formatted_input).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= MinMaxScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "               \n",
    "        if isinstance(layer, nn.MaxPool2d): \n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "            \n",
    "            if name in style_layers:\n",
    "                target_feature = model(formatted_input).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                target_feature_gram=torch.flatten(target_feature_gram)\n",
    "                target_feature_gram=target_feature_gram.numpy()\n",
    "                target_feature_gram=target_feature_gram.reshape(512,512)\n",
    "                target_feature_gram= MinMaxScaler().fit_transform(target_feature_gram)\n",
    "                result.append((target_feature_gram))\n",
    "                \n",
    "        i += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbf210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222b29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ba6e6f1",
   "metadata": {},
   "source": [
    "## Define Gram Loss (normalized w.r.t a target as defined in Antognini et. al)\n",
    "\n",
    "### Based on the normalized gram loss defined in Antognini et al. (paper link at the top of this notebook)\n",
    "\n",
    "#### Find Gram loss between two Gram Matrices. Normalized by the 'referenced' or 'target' gram matrix\n",
    "\n",
    "$$Gram\\_Loss_{G, \\hat{G}} = \\frac{\\sum_{k,\\mu,v}(G_{\\mu,v}^k - \\hat{G}_{\\mu,v}^k)^2}{\\sum_{k,\\mu,v}(\\hat{G}_{\\mu,v}^k)^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bac3df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antognini_gram_loss(gm_1, gm_2):\n",
    "    # Here both gm_1 and gm_2 are of shape 6 X 1 X 512 X 512\n",
    "    frobenius_sum_of_gm_diff = 0\n",
    "    frobenius_sum_of_target_norm = 0\n",
    "    for layer_num in range(gm_1.shape[0]):\n",
    "        frobenius_sum_of_gm_diff += np.linalg.norm(gm_1[layer_num][0] - gm_2[layer_num][0])**2\n",
    "        frobenius_sum_of_target_norm += np.linalg.norm(gm_2[layer_num][0])**2\n",
    "    return frobenius_sum_of_gm_diff/frobenius_sum_of_target_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1dec318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cos_distance(gram1,gram2): \n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    gram1_ = np.squeeze(gram1, axis=1)\n",
    "    gram2_ = np.squeeze(gram2, axis=1)\n",
    "    distance = np.zeros((6,1))\n",
    "    for i in range(6): \n",
    "        temp= cos(torch.from_numpy(gram1_[i].flatten()), torch.from_numpy(gram2_[i].flatten())) \n",
    "        distance[i]=temp\n",
    "    return 1-distance.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706c8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlist = initialize_parallel_cnns(N_PARALLEL_CNNS, N_FREQ, N_FILTERS, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d39993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06635b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We currently use compute_cos_distance. This can be easily switched to using get_antognini_gram_loss\n",
    "#\n",
    "def get_param_sense_losses(audio_dir, is_gan=True, num_interp_steps=10):\n",
    "    example_dirs = [_ for _ in os.listdir(audio_dir) if _ !='.DS_Store']\n",
    "    print(example_dirs)\n",
    "    all_gm_losses = np.empty((0,num_interp_steps))\n",
    "    for example_dir in example_dirs:\n",
    "\n",
    "        print('Analysing example ', example_dir)\n",
    "        if is_gan:\n",
    "            example_loc_path = os.path.join(audio_dir, example_dir, 'one_z_pitch_sweep')\n",
    "        else:\n",
    "            example_loc_path = os.path.join(audio_dir, example_dir)\n",
    "\n",
    "        example_loc = os.listdir(example_loc_path)\n",
    "        example_loc = [_ for _ in os.listdir(example_loc_path) if '.wav' in _]\n",
    "        if is_gan:\n",
    "            example_loc.sort(key=lambda x:int(x.split('_')[3].split('.')[0]))\n",
    "        else:\n",
    "            example_loc.sort(key=lambda x:float(x.split('_')[1].split('.wav')[0]))\n",
    "        #print(example_loc)\n",
    "        audio_file_list = []\n",
    "        for example_audio in example_loc:\n",
    "            if '.wav' in example_audio and example_audio:\n",
    "                audio_file_list.append(os.path.join(example_loc_path, example_audio))\n",
    "        \n",
    "        audio_0 = prepare_input(audio_file_list[0])\n",
    "        audio_0_gm = []\n",
    "        for j in range(N_PARALLEL_CNNS):\n",
    "            temp = get_gram(cnnlist[j], audio_0, style_layers=style_layers_default)\n",
    "            audio_0_gm.append(temp)\n",
    "        audio_0_gm = np.array(audio_0_gm)\n",
    "\n",
    "        example_gm_losses = np.array([])\n",
    "        for audio_file in audio_file_list:\n",
    "            audio_i = prepare_input(audio_file)\n",
    "\n",
    "            audio_i_gm = []\n",
    "            for j in range(N_PARALLEL_CNNS):\n",
    "                temp = get_gram(cnnlist[j], audio_i, style_layers=style_layers_default)\n",
    "                audio_i_gm.append(temp)\n",
    "            audio_i_gm = np.array(audio_i_gm)\n",
    "            example_gm_losses = np.append(example_gm_losses, compute_cos_distance(audio_0_gm, audio_i_gm))\n",
    "        #print(example_gm_losses)\n",
    "        all_gm_losses = np.append(all_gm_losses, np.array([example_gm_losses]), axis=0)\n",
    "\n",
    "    print('GM Loss array', np.mean(all_gm_losses, axis=0))\n",
    "    return np.mean(all_gm_losses, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d18720a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05377e60",
   "metadata": {},
   "source": [
    "# MorphGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f247fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-10-17 22:50', '2022-10-17 22:51', '2022-10-17 22:56', '2022-10-17 22:45', '2022-10-17 22:54', '2022-10-17 22:55', '2022-10-17 22:52', '2022-10-17 22:46', '2022-10-17 22:49', '2022-10-17 22:47']\n",
      "Analysing example  2022-10-17 22:50\n",
      "Analysing example  2022-10-17 22:51\n",
      "Analysing example  2022-10-17 22:56\n",
      "Analysing example  2022-10-17 22:45\n",
      "Analysing example  2022-10-17 22:54\n",
      "Analysing example  2022-10-17 22:55\n",
      "Analysing example  2022-10-17 22:52\n",
      "Analysing example  2022-10-17 22:46\n",
      "Analysing example  2022-10-17 22:49\n",
      "Analysing example  2022-10-17 22:47\n",
      "GM Loss array [0.         0.03500256 0.07726649 0.14428613 0.22187538 0.27423\n",
      " 0.29757555 0.30237291 0.32440894 0.34319202 0.35346957]\n",
      "MorphGAN: Corr coef for Morph Param Sensitivity =  (0.9610206267161887, 2.522054880006216e-06)\n"
     ]
    }
   ],
   "source": [
    "morphgan_audio='/Users/purnimakamath/appdir/Github/ieee-tx-on-mm/data/water-wind-new/morphgan/audio/morph'\n",
    "morphgan_paramsense_losses = get_param_sense_losses(morphgan_audio, is_gan=True, num_interp_steps=11)\n",
    "\n",
    "lin_line = [a for a in np.arange(0,1.1,0.1)]\n",
    "print('MorphGAN: Corr coef for Morph Param Sensitivity = ', pearsonr(lin_line, morphgan_paramsense_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6f494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0286721",
   "metadata": {},
   "source": [
    "# One Hot GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b5536b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-09-02 15:13', '2022-09-02 15:14', '2022-09-02 15:15', '2022-09-02 15:12', '2022-09-02 15:09', '2022-09-02 15:19', '2022-09-02 15:21', '2022-09-02 15:17', '2022-09-02 15:10', '2022-09-02 15:18']\n",
      "Analysing example  2022-09-02 15:13\n",
      "Analysing example  2022-09-02 15:14\n",
      "Analysing example  2022-09-02 15:15\n",
      "Analysing example  2022-09-02 15:12\n",
      "Analysing example  2022-09-02 15:09\n",
      "Analysing example  2022-09-02 15:19\n",
      "Analysing example  2022-09-02 15:21\n",
      "Analysing example  2022-09-02 15:17\n",
      "Analysing example  2022-09-02 15:10\n",
      "Analysing example  2022-09-02 15:18\n",
      "GM Loss array [0.         0.01987916 0.03990053 0.06795186 0.1137172  0.18776335\n",
      " 0.25538763 0.27665292 0.27850003 0.28538674 0.28483344]\n",
      "One Hot: Corr coef for Morph Param Sensitivity =  (0.9622982016888724, 2.174868328545306e-06)\n"
     ]
    }
   ],
   "source": [
    "onehot_audio='/Users/purnimakamath/appdir/Github/ieee-tx-on-mm/data/water-wind/onehot/audio/morph'\n",
    "onehot_paramsense_losses = get_param_sense_losses(onehot_audio, is_gan=True, num_interp_steps=11)\n",
    "\n",
    "lin_line = [a for a in np.arange(0,1.1,0.1)]\n",
    "print('One Hot: Corr coef for Morph Param Sensitivity = ', pearsonr(lin_line, onehot_paramsense_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c3e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e08c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80ffb4e0",
   "metadata": {},
   "source": [
    "# Morph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57827ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '0', '7', '6', '1', '10', '8', '4', '3', '2', '5']\n",
      "Analysing example  9\n",
      "Analysing example  0\n",
      "Analysing example  7\n",
      "Analysing example  6\n",
      "Analysing example  1\n",
      "Analysing example  10\n",
      "Analysing example  8\n",
      "Analysing example  4\n",
      "Analysing example  3\n",
      "Analysing example  2\n",
      "Analysing example  5\n",
      "GM Loss array [0.         0.18931039 0.19119945 0.22618087 0.34717031 0.39623025\n",
      " 0.42982481 0.45498322 0.46450749 0.46383756 0.33671759]\n",
      "Morph2: Corr coef for Morph Param Sensitivity =  (0.8353060703884401, 0.0013701879567764117)\n"
     ]
    }
   ],
   "source": [
    "morph2_audio='/Users/purnimakamath/appdir/Github/ieee-tx-on-mm/data/water-wind/morph2/audio/morph'\n",
    "morph2_paramsense_losses = get_param_sense_losses(morph2_audio, is_gan=False, num_interp_steps=11)\n",
    "\n",
    "lin_line = [a for a in np.arange(0,1.1,0.1)]\n",
    "print('Morph2: Corr coef for Morph Param Sensitivity = ', pearsonr(lin_line, morph2_paramsense_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f213a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c2ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
